---
title: 浅浅理顺一下深度学习在干嘛
date: 2023-02-15 22:00:00
author: Cardioid
top: false
hide: false
cover: false
toc: false
mathjax: true
summary: 深度学习原理初步
categories: 
tags: 计算机与安全理论
---
FROM. 李宏毅老师的网上教程。
# 一、人生意义：找函数罢了
深度学习找一个函数$y=f(x)$。其中$x$和$y$可以是数值、序列、自然语言、图片视频音频等等。
深度学习用来回归（给一个$x$得一个$y$）、分类、创作......
# 二、干活的准则：损失函数
MAE：$e=|y-\widehat{y}|$；MSE：$e=(y-\widehat{y})^2$。
损失Loss为$L=\frac{1}{n}\sum_{n}e_n$。
# 三、谁来指引我干活：梯度下降
函数在某点的梯度是一个向量，它的方向与取得最大方向导数的方向一致，它的模为方向导数的最大值。
所以沿梯度方向，损失函数Loss咵咵咵降得最快，能尽早降到导数值是0的点，也就找到了损失函数的极小值点。
每次算完梯度$\nabla$后，$w(n+1)=w(n)-\eta\frac{dLoss}{dw(n)}$，超参$\eta$是学习率。
$\theta(n+1)=argmin_{\theta}(L)$。
# 四、计算机怎么算梯度：反向传播#
网络从前往后搭，梯度从后往前算。
（具体而言就是算链式法则求导，理解起来不难，但不想写了，写到现在都0:25了，开摆！）
# 五、刻画函数：激活函数#
Sigmoid：$y=c\frac{1}{1+e^{-(b+wx)}}$，用一些Sigmoid的线性组合可以组成各式各样的函数；
ReLU：$y=c max(0,b+wx)$，用一些ReLU的线性组合可以组成各式各样的函数。
# 六、神经元在工作#
$y=b+\sum_{i}c_i sigmoid(b_i+\sum_{j}w_{ij}x_j)$，
即，$y=b_{0}+C^T\sigma(b+wx)$。
# 七、内存装不下#
batch：每次读入一个batch，然后根据这一个batch的数据往梯度下降的方向来调整网络中的各种参数，使得损失函数的值尽可能小。
epoch：跑完所有batch是一轮epoch。
# 八、搭建深度学习神经网络#
堆更多的隐藏层，小心过拟合哈。
